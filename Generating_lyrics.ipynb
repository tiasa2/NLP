{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating_lyrics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZV85aFViVoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys \n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Activation, Flatten, Dropout, Dense, Embedding, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY_O0KQviz75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b4e9fe1c-fcf3-4d94-a3dc-48446a99d157"
      },
      "source": [
        "#Load the dataset\n",
        "dataset = pd.read_csv('tsl.csv', encoding = \"latin1\")\n",
        "dataset.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>album</th>\n",
              "      <th>track_title</th>\n",
              "      <th>track_n</th>\n",
              "      <th>lyric</th>\n",
              "      <th>line</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>He said the way my blue eyes shined</td>\n",
              "      <td>1</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>Put those Georgia stars to shame that night</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>I said, \"That's a lie\"</td>\n",
              "      <td>3</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>Just a boy in a Chevy truck</td>\n",
              "      <td>4</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>That had a tendency of gettin' stuck</td>\n",
              "      <td>5</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist         album  ... line  year\n",
              "0  Taylor Swift  Taylor Swift  ...    1  2006\n",
              "1  Taylor Swift  Taylor Swift  ...    2  2006\n",
              "2  Taylor Swift  Taylor Swift  ...    3  2006\n",
              "3  Taylor Swift  Taylor Swift  ...    4  2006\n",
              "4  Taylor Swift  Taylor Swift  ...    5  2006\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkVcFxv3l3TP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processFirstLine(lyrics, songID, songName, row):\n",
        "    lyrics.append(row['lyric'] + '\\n')\n",
        "    songID.append( row['year']*100+ row['track_n'])\n",
        "    songName.append(row['track_title'])\n",
        "    return lyrics,songID,songName\n",
        "# define empty lists for the lyrics , songID , songName \n",
        "lyrics = []\n",
        "songID = []\n",
        "songName = []\n",
        "# songNumber indicates the song number in the dataset\n",
        "songNumber = 1\n",
        "# i indicates the song number\n",
        "i = 0\n",
        "isFirstLine = True\n",
        "# Iterate through every lyrics line and join them together for each song independently \n",
        "for index,row in dataset.iterrows():\n",
        "    if(songNumber == row['track_n']):\n",
        "        if (isFirstLine):\n",
        "            lyrics,songID,songName = processFirstLine(lyrics,songID,songName,row)\n",
        "            isFirstLine = False\n",
        "        else :\n",
        "            #if we still in the same song , keep joining the lyrics lines    \n",
        "            lyrics[i] +=  row['lyric'] + '\\n'\n",
        "    #When it's done joining a song's lyrics lines , go to the next song :    \n",
        "    else :\n",
        "        lyrics,songID,songName = processFirstLine(lyrics,songID,songName,row)\n",
        "        songNumber = row['track_n']\n",
        "        i+=1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYO6EmF8l54E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyrics_data = pd.DataFrame({'songID':songID, 'songName':songName, 'lyrics':lyrics })"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_x4An6gmC3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('lyricsText.txt', 'w',encoding=\"utf-8\") as filehandle:  \n",
        "    for listitem in lyrics:\n",
        "        filehandle.write('%s\\n' % listitem)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoZyxz4imE9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textFileName = 'lyricsText.txt'\n",
        "raw_text = open(textFileName, encoding = 'UTF-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2rd5VBwmHTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "int_chars = dict((i, c) for i, c in enumerate(chars))\n",
        "chars_int = dict((i, c) for c, i in enumerate(chars))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pptnWxfkmOSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-TDvzgmQti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28110ccb-2223-45c9-8504-44a256452c92"
      },
      "source": [
        "seq_len = 100\n",
        "data_X = []\n",
        "data_y = []\n",
        "for i in range(0, n_chars - seq_len, 1):\n",
        "    # Input Sequeance(will be used as samples)\n",
        "    seq_in  = raw_text[i:i+seq_len]\n",
        "    # Output sequence (will be used as target)\n",
        "    seq_out = raw_text[i + seq_len]\n",
        "    # Store samples in data_X\n",
        "    data_X.append([chars_int[char] for char in seq_in])\n",
        "    # Store targets in data_y\n",
        "    data_y.append(chars_int[seq_out])\n",
        "n_patterns = len(data_X)\n",
        "print( 'Total Patterns : ', n_patterns)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns :  173598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcZBfDi5mUrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.reshape(data_X , (n_patterns, seq_len, 1))\n",
        "# Normalizing input data :\n",
        "X = X/ float(n_vocab)\n",
        "# One hot encode the output targets :\n",
        "y = np_utils.to_categorical(data_y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fg8tJ3dmYrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_layer_num = 4 # number of LSTM layers\n",
        "layer_size = [256,256,256,256] # number of nodes in each layer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQrdQ2HQmcKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I32LzpNmeUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(LSTM(layer_size[0], input_shape =(X.shape[1], X.shape[2]), return_sequences = True))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5WutwjnmijK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1,LSTM_layer_num) :\n",
        "    model.add(LSTM(layer_size[i], return_sequences=True))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fXTvWuamoLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Flatten())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIad4Brbmqnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(y.shape[1]))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goThrEWumuJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "944dffe2-f216-4612-806b-9e695da857b4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100, 256)          525312    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100, 256)          525312    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100, 256)          525312    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25600)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 58)                1484858   \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 58)                0         \n",
            "=================================================================\n",
            "Total params: 3,324,986\n",
            "Trainable params: 3,324,986\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsw7XbJ_mwbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-LSTM-improvement-{epoch:03d}-{loss:.5f}-bigger.hdf5'\n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='loss', verbose = 1, save_best_only = True, mode ='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn9uXbwimy2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3c63551-7869-435c-a4bd-51c086d9798d"
      },
      "source": [
        "model_params = {'epochs':15,\n",
        "                'batch_size':128,\n",
        "                'callbacks':callbacks_list,\n",
        "                'verbose':1,\n",
        "                'validation_split':0.2,\n",
        "                'validation_data':None,\n",
        "                'shuffle': True,\n",
        "                'initial_epoch':0,\n",
        "                'steps_per_epoch':None,\n",
        "                'validation_steps':None}\n",
        "model.fit(X,\n",
        "          y,\n",
        "          epochs = model_params['epochs'],\n",
        "           batch_size = model_params['batch_size'],\n",
        "           callbacks= model_params['callbacks'],\n",
        "           verbose = model_params['verbose'],\n",
        "           validation_split = model_params['validation_split'],\n",
        "           validation_data = model_params['validation_data'],\n",
        "           shuffle = model_params['shuffle'],\n",
        "           initial_epoch = model_params['initial_epoch'],\n",
        "           steps_per_epoch = model_params['steps_per_epoch'],\n",
        "           validation_steps = model_params['validation_steps'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 2.5182\n",
            "Epoch 00001: loss improved from 2.71296 to 2.51822, saving model to Weights-LSTM-improvement-001-2.51822-bigger.hdf5\n",
            "1085/1085 [==============================] - 78s 72ms/step - loss: 2.5182 - val_loss: 2.6276\n",
            "Epoch 2/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 2.2302\n",
            "Epoch 00002: loss improved from 2.51822 to 2.23025, saving model to Weights-LSTM-improvement-002-2.23025-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 72ms/step - loss: 2.2302 - val_loss: 2.6151\n",
            "Epoch 3/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 1.8622\n",
            "Epoch 00003: loss improved from 2.23025 to 1.86221, saving model to Weights-LSTM-improvement-003-1.86221-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 72ms/step - loss: 1.8622 - val_loss: 2.6136\n",
            "Epoch 4/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 1.4791\n",
            "Epoch 00004: loss improved from 1.86221 to 1.47906, saving model to Weights-LSTM-improvement-004-1.47906-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 1.4791 - val_loss: 2.7843\n",
            "Epoch 5/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 1.1348\n",
            "Epoch 00005: loss improved from 1.47906 to 1.13481, saving model to Weights-LSTM-improvement-005-1.13481-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 1.1348 - val_loss: 2.9983\n",
            "Epoch 6/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.8348\n",
            "Epoch 00006: loss improved from 1.13481 to 0.83476, saving model to Weights-LSTM-improvement-006-0.83476-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 72ms/step - loss: 0.8348 - val_loss: 3.3450\n",
            "Epoch 7/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.5891\n",
            "Epoch 00007: loss improved from 0.83476 to 0.58913, saving model to Weights-LSTM-improvement-007-0.58913-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 72ms/step - loss: 0.5891 - val_loss: 3.7310\n",
            "Epoch 8/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.3970\n",
            "Epoch 00008: loss improved from 0.58913 to 0.39696, saving model to Weights-LSTM-improvement-008-0.39696-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 0.3970 - val_loss: 4.1722\n",
            "Epoch 9/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.2676\n",
            "Epoch 00009: loss improved from 0.39696 to 0.26759, saving model to Weights-LSTM-improvement-009-0.26759-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 0.2676 - val_loss: 4.5388\n",
            "Epoch 10/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.1938\n",
            "Epoch 00010: loss improved from 0.26759 to 0.19380, saving model to Weights-LSTM-improvement-010-0.19380-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 0.1938 - val_loss: 4.9451\n",
            "Epoch 11/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.1530\n",
            "Epoch 00011: loss improved from 0.19380 to 0.15304, saving model to Weights-LSTM-improvement-011-0.15304-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 0.1530 - val_loss: 5.1750\n",
            "Epoch 12/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.1389\n",
            "Epoch 00012: loss improved from 0.15304 to 0.13893, saving model to Weights-LSTM-improvement-012-0.13893-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 0.1389 - val_loss: 5.3416\n",
            "Epoch 13/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.1210\n",
            "Epoch 00013: loss improved from 0.13893 to 0.12104, saving model to Weights-LSTM-improvement-013-0.12104-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 0.1210 - val_loss: 5.6104\n",
            "Epoch 14/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.1114\n",
            "Epoch 00014: loss improved from 0.12104 to 0.11136, saving model to Weights-LSTM-improvement-014-0.11136-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 0.1114 - val_loss: 5.7164\n",
            "Epoch 15/15\n",
            "1085/1085 [==============================] - ETA: 0s - loss: 0.1078\n",
            "Epoch 00015: loss improved from 0.11136 to 0.10778, saving model to Weights-LSTM-improvement-015-0.10778-bigger.hdf5\n",
            "1085/1085 [==============================] - 79s 73ms/step - loss: 0.1078 - val_loss: 5.6379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fead0c51080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlXc4Ghmm2RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load wights file :\n",
        "wights_file = '/content/Weights-LSTM-improvement-015-0.10778-bigger.hdf5' # weights file path\n",
        "model.load_weights(wights_file)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM3e37d8vJZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "9fcfdccf-8374-472f-d848-fb1d62784c99"
      },
      "source": [
        "# set a random seed :\n",
        "start = np.random.randint(0, len(data_X)-1)\n",
        "pattern = data_X[start]\n",
        "print('Seed : ')\n",
        "print(\"\\\"\",''.join([int_chars[value] for value in pattern]), \"\\\"\\n\")\n",
        "# How many characters you want to generate\n",
        "generated_characters = 500\n",
        "# Generate Charachters :\n",
        "for i in range(generated_characters):\n",
        "    x = np.reshape(pattern, ( 1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x,verbose = 0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_chars[index]\n",
        "    #seq_in = [int_chars[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print('\\nDone')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed : \n",
            "\" d i got that red lip classic thing that you like\n",
            "and when we go crashing down, we come back everytim \"\n",
            "\n",
            "e\n",
            "cause we never go out of style, we never go out of style\n",
            "\n",
            "looking at it now\n",
            "it all seems so simple\n",
            "we were lying on the stouch\n",
            "i remembe\n",
            "that took a polaroid of us\n",
            "then discovered it ereais of doon\n",
            "eienies to keav no the meget, eoe io my miae\n",
            "the say, yea,\n",
            "yeshou  you siool them cowe to a fay mookes, tteat on a cho ond rntt\n",
            "iete \n",
            "nohane au lorn oa inmemanf\n",
            "i dan't halp it onsk tp i knoe syartsiiggt ailcy i can't fes fetan miss you worln\n",
            "ort oosking himl\n",
            "it's worle i know you\n",
            "cue iimoing and yo\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}